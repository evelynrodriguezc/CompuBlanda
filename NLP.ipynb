{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trabajo 1 de compu blanda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evelynrodriguezc/computacion-blanda/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF_ODThuxzmZ"
      },
      "source": [
        "#¿ QUÉ ES EL PROCESAMIENTO DE LENGUAJE NATURAL O NLP ?\n",
        "\n",
        "Es el conjunto de tecnicas de inteligencia artificial orientadas al manejo o entendimiento del lenguaje, entiendase lenguaje la forma en la que nos comunicamos los humanos , se le conce como NLP por sus siglas en ingles Natural Languaje Processing, su algoritmo mas conocido el algoritmo de textrank usado para realizar resumenes de texto automatizados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGy-ZErbyOSS"
      },
      "source": [
        "## Aplicaciones\n",
        "\n",
        "el NLP es usado en diversas areas, como auditorias, chatbots, traductores, resumenes automatizados, recuperacion de informacion, asistentes por voz entre muchas otras areas.\n",
        "\n",
        "pero este tiene varios detalles de implementacion como lo pueden ser el idioma, el contexto del uso del lenguaje, o en algunos casos el coste computacional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9G_Ys0AyQiK"
      },
      "source": [
        "## Técnicas\n",
        "\n",
        "El NLP requiere de ciertas base conocimiento en matematicas,estadistica, linguistica y computacion, para al finall legar un conjunto de pasos o algoritmos especificos usados en conjunto para el procesamiento del lenguaje, algunos de estas tecnicas son:\n",
        "\n",
        "1. **Tokenizacion ->** Es la forma en la cual un texto se divide en nucleos textuales individuales, es decir palabras u oraciones acorde a la necesidad.\n",
        "\n",
        "2. **Lemmatizacion ->** Es la tecnica mediante la cual un texto se lleva a su representacion mas basica del lenguaje es decir poner todos los sujetos en singular y los verbos en infitivo. \n",
        "\n",
        "3. **Limpieza de Stop Words ->** Es el proceso mediante el cual se quitan todas las palabras que no aportan valor al lenguaje, como lo son conectores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Nug-yW2i-3"
      },
      "source": [
        "# Ejemplo de tokenizacion \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMC2Xfaf18OU",
        "outputId": "66a7ced8-3045-48e7-9dec-8e591353f8bd"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "texto_tokenizar = \"hola soy un texto de prueba que,va ser tokenizado :3\"\n",
        "tokens = nltk.word_tokenize(texto_tokenizar)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hola',\n",
              " 'soy',\n",
              " 'un',\n",
              " 'texto',\n",
              " 'de',\n",
              " 'prueba',\n",
              " 'que',\n",
              " ',',\n",
              " 'va',\n",
              " 'ser',\n",
              " 'tokenizado',\n",
              " ':3']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ww5dfn93Y0c"
      },
      "source": [
        "# Ejemplo de limpieza de stopword\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XXihCNG3YVC",
        "outputId": "09805f49-5409-4eeb-ac0b-38d596181cda"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words(\"spanish\"))\n",
        "for token in tokens:\n",
        "  if token in stops or len(token) <= 2:\n",
        "    tokens.remove(token)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "{'serías', 'tuvisteis', 'poco', 'seréis', 'tengas', 'estaríais', 'tenemos', 'suya', 'éramos', 'tuviera', 'seremos', 'estas', 'que', 'estos', 'hubieron', 'quien', 'mía', 'desde', 'vuestros', 'fuimos', 'tengamos', 'seas', 'hubisteis', 'les', 'nos', 'hubieras', 'sentida', 'fuéramos', 'otras', 'estuvieron', 'seamos', 'ellos', 'pero', 'mí', 'habéis', 'estuvieses', 'estuvieseis', 'estuve', 'serán', 'nosotros', 'habremos', 'ha', 'hasta', 'esta', 'antes', 'somos', 'estaría', 'sentidos', 'había', 'los', 'seré', 'estaré', 'seáis', 'más', 'eso', 'habidas', 'eras', 'ti', 'hayan', 'el', 'como', 'tenían', 'estarás', 'teníais', 'nosotras', 'sentido', 'en', 'estuvisteis', 'estado', 'estás', 'tendríais', 'nuestra', 'habrían', 'fueron', 'fuesen', 'hay', 'sus', 'donde', 'estada', 'fuerais', 'estad', 'contra', 'os', 'tanto', 'soy', 'sería', 'hube', 'vosotras', 'él', 'tenido', 'lo', 'ante', 'estuviesen', 'siente', 'estuviese', 'estuvimos', 'esos', 'tendríamos', 'estáis', 'fueses', 'tienes', 'tus', 'mías', 'suyo', 'tenidos', 'sois', 'fuese', 'ese', 'tenía', 'e', 'sí', 'fueras', 'qué', 'tengan', 'vuestra', 'mucho', 'será', 'estuvo', 'tendrías', 'tuvieseis', 'al', 'hubieran', 'habríamos', 'serían', 'tuyo', 'tendrás', 'suyas', 'cual', 'de', 'sin', 'ni', 'tuviese', 'es', 'seríamos', 'habría', 'habido', 'hubiésemos', 'estuvieras', 'estuvierais', 'esa', 'una', 'tuviésemos', 'habrías', 'vosotros', 'vuestro', 'tenidas', 'tened', 'habríais', 'estuviera', 'estés', 'porque', 'entre', 'sentidas', 'estaréis', 'fuera', 'esas', 'estábamos', 'tenida', 'muchos', 'fuésemos', 'habiendo', 'has', 'míos', 'nada', 'hayas', 'habrá', 'estados', 'las', 'hubieses', 'era', 'estemos', 'tendrá', 'tenga', 'un', 'estuviste', 'sentid', 'estaba', 'hayáis', 'estarán', 'habrán', 'vuestras', 'hemos', 'cuando', 'habidos', 'sean', 'estaban', 'tuvieras', 'estaremos', 'sobre', 'tienen', 'habré', 'estaríamos', 'tiene', 'otros', 'mi', 'ellas', 'estabas', 'ya', 'estar', 'por', 'le', 'tuvierais', 'estarías', 'nuestras', 'estabais', 'algunos', 'tendrán', 'algunas', 'hayamos', 'fuisteis', 'mío', 'han', 'serás', 'tengáis', 'hubo', 'haya', 'estén', 'quienes', 'estadas', 'me', 'hubimos', 'son', 'habida', 'estará', 'nuestros', 'he', 'hubierais', 'fui', 'otra', 'tuviste', 'uno', 'eres', 'fueran', 'tendré', 'yo', 'tuvo', 'tengo', 'tuvieran', 'tenías', 'no', 'a', 'habrás', 'habías', 'tenéis', 'hubiste', 'unos', 'esté', 'eran', 'fue', 'fueseis', 'hubiera', 'habíais', 'erais', 'con', 'están', 'seríais', 'tuyas', 'habréis', 'tuve', 'estuviésemos', 'tendrían', 'hubieseis', 'o', 'tendría', 'para', 'este', 'fuiste', 'suyos', 'te', 'habíamos', 'estuviéramos', 'hubiéramos', 'y', 'estamos', 'teniendo', 'tuviéramos', 'tendréis', 'otro', 'la', 'tendremos', 'mis', 'tuviesen', 'está', 'tuya', 'todos', 'hubiese', 'estuvieran', 'estoy', 'del', 'habían', 'se', 'su', 'sea', 'tuvieses', 'tuyos', 'durante', 'tú', 'sintiendo', 'teníamos', 'también', 'muy', 'todo', 'ella', 'tu', 'tuvimos', 'nuestro', 'tuvieron', 'esto', 'hubiesen', 'estarían', 'algo', 'estando', 'estéis'}\n",
            "['hola', 'texto', 'prueba', 'va', 'ser', 'tokenizado']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adl6LdS3yS09"
      },
      "source": [
        "# Machine learning en NLP\n",
        "Las tecnicas de machine learning o deep learning mas usadas en en nlp suele ir orientadas a aplicaciones de resumenes , traduccion o chatbot.\n",
        "\n",
        "En la aplicacion para resumenes se usa para casos de resumen de tipo abstracto o de construccion propia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOrpIDwYQwiq"
      },
      "source": [
        "#Ejemplo etiquetador sentimiento con Ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SggP7AGOQ3zB"
      },
      "source": [
        "### Paso 1\n",
        "- importar librerias\n",
        "- crear la base entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvLKydTxKrfF"
      },
      "source": [
        "# Librerías\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "# definir documentos\n",
        "docs = ['te quiero',\n",
        "        'soy feliz',\n",
        "        'genial!!',\n",
        "        'excelente servicio',\n",
        "        'wow esto es increible',\n",
        "        'meh',\n",
        "        'por qué naci',\n",
        "        'te odio',\n",
        "        'me das rabia',\n",
        "        'creo que si pero no']\n",
        "\n",
        "testdocs = ['que feliz me siento hoy',\n",
        "        'tuve un dia lindo','odio existir']\n",
        "\n",
        "# definir rótulos de clase\n",
        "labels = array([1,1,1,1,1,0,0,-1,-1,-1])\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDPYRkqXRCiP"
      },
      "source": [
        "## Paso 2 \n",
        "- tokenizar las palabras\n",
        "- normalizar los datos para su uso en la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8VkhYJ7Q_FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4bc012-c14a-4e55-b085-c7a34e5c278e"
      },
      "source": [
        "\n",
        "# enteros codifican los documentos\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "encoded_testdocs = [one_hot(d, vocab_size) for d in testdocs]\n",
        "print(encoded_docs)\n",
        "print(encoded_testdocs)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[32, 23], [15, 26], [1], [42, 7], [27, 17, 41, 43], [18], [30, 26, 34], [32, 3], [6, 46, 11], [39, 36, 37, 2, 3]]\n",
            "[[36, 26, 6, 3, 35], [12, 44, 30, 23], [3, 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmxuTbGoROJn"
      },
      "source": [
        "## Paso 3 \n",
        "- preparar los datos para la red (es decir en forma matricial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0910zfhSLRIp",
        "outputId": "0a733ced-2bbc-49ff-f56e-f561ae0e7220"
      },
      "source": [
        "# documentos pad con una longitud máxima de 4 palabras\n",
        "max_length = 5\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "padded_testdocs = pad_sequences(encoded_testdocs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "print(padded_testdocs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[36 35  0  0  0]\n",
            " [10 42  0  0  0]\n",
            " [ 9  0  0  0  0]\n",
            " [16 29  0  0  0]\n",
            " [25 33 10 22  0]\n",
            " [43  0  0  0  0]\n",
            " [34  5 25  0  0]\n",
            " [36 22  0  0  0]\n",
            " [ 5 43 21  0  0]\n",
            " [17 35  4 21 19]]\n",
            "[[35 42  5 37  2]\n",
            " [26 29 26 42  0]\n",
            " [22  2  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STfvXrVHRUxX"
      },
      "source": [
        "## Paso 4 \n",
        "- diseñar y construir el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvgnCTb4Lh7b"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='relu'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH_6fImLRdMp"
      },
      "source": [
        "## Paso 5 \n",
        "- entrenar el modelo con los datos pruebas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u19Cn12EOW9M",
        "outputId": "fd72a5f3-05b4-4412-9062-5b792c92ce54"
      },
      "source": [
        "model.fit(padded_docs, labels, epochs=30, verbose=0)\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "loss,accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79d607e3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsSjvpA-Rl0c"
      },
      "source": [
        "## Paso 6 \n",
        "probar el modelo con los datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqvFB3PYPUXE",
        "outputId": "3b419921-00de-422f-ecbb-fe022e50b6d9"
      },
      "source": [
        "a = model.predict(padded_testdocs)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01332596],\n",
              "       [0.07693202],\n",
              "       [0.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}